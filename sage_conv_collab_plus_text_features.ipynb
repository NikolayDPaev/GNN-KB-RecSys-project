{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from IPython import get_ipython # type: ignore\n",
    "import os\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# get the notebook name\n",
    "ip = get_ipython()\n",
    "path = None\n",
    "if '__vsc_ipynb_file__' in ip.user_ns: # type: ignore\n",
    "    path = ip.user_ns['__vsc_ipynb_file__'] # type: ignore\n",
    "\n",
    "os.makedirs('models/', exist_ok=True)\n",
    "model_file_name = f\"models/{os.path.basename(path)[:-6]}.pt\" # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = 'dataset/ml-latest-small/movies_with_wikidata_values.csv'\n",
    "rating_path = 'dataset/ml-latest-small/ratings.csv'\n",
    "\n",
    "ratings_df = pd.read_csv(rating_path)[[\"userId\", \"movieId\", \"rating\"]]\n",
    "movies_df = pd.read_csv(movie_path, index_col='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11347e62aee46c98d9cdb4b4e1b87ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encode the genres:\n",
    "genres = movies_df['genres'].str.get_dummies('|').values\n",
    "genres = torch.from_numpy(\n",
    "    genres\n",
    "    ).to(torch.float)\n",
    "\n",
    "years = torch.nan_to_num(torch.from_numpy(\n",
    "    movies_df['year'].values\n",
    "    ).to(torch.float), nan=1990).unsqueeze(1)\n",
    "box_office = torch.nan_to_num(torch.from_numpy(\n",
    "    movies_df['boxOfficeWorldwide'].map(lambda x: x/1000000).values\n",
    "    ).to(torch.float), nan=0).unsqueeze(1)\n",
    "score = torch.from_numpy(\n",
    "    movies_df['tommatometerScore'].map(lambda x: int(x[:-1] if isinstance(x, str) and len(x) > 1 else 50)).values\n",
    "    ).to(torch.float).unsqueeze(1)\n",
    "duration = torch.nan_to_num(torch.from_numpy(\n",
    "    movies_df['duration'].values\n",
    "    ).to(torch.float), nan=70).unsqueeze(1)\n",
    "\n",
    "# Load the pre-trained sentence transformer model and encode the movie titles:\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "with torch.no_grad():\n",
    "    titles = model.encode(movies_df['title'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    titles = titles.cpu()\n",
    "\n",
    "# Concatenate the genres and title features:\n",
    "movie_ids = torch.LongTensor(range(len(movies_df))).unsqueeze(1)\n",
    "movie_features = torch.cat([movie_ids, genres, titles], dim=-1)\n",
    "\n",
    "# We don't have user features, which is why we use an identity matrix\n",
    "user_ids_tensor = torch.LongTensor(range(len(ratings_df['userId'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_id = ratings_df['userId'].unique()\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'userId': unique_user_id,\n",
    "    'mappedUserId': pd.RangeIndex(len(unique_user_id))\n",
    "    })\n",
    "\n",
    "# Create a mapping from the movieId to a unique consecutive value in the range [0, num_movies]:\n",
    "unique_movie_id = ratings_df['movieId'].unique()\n",
    "unique_movie_id = pd.DataFrame(data={\n",
    "    'movieId': unique_movie_id,\n",
    "    'mappedMovieId': pd.RangeIndex(len(unique_movie_id))\n",
    "    })\n",
    "\n",
    "# Merge the mappings with the original data frame:\n",
    "ratings_df = ratings_df.merge(unique_user_id, on='userId')\n",
    "ratings_df = ratings_df.merge(unique_movie_id, on='movieId')\n",
    "\n",
    "ratings_df_train, ratings_df_test = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "ratings_df_test, ratings_df_val = train_test_split(ratings_df_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(ratings_df):\n",
    "    edge_index = torch.stack([\n",
    "    torch.tensor(ratings_df['mappedUserId'].values),\n",
    "    torch.tensor(ratings_df['mappedMovieId'].values)]\n",
    "    , dim=0)\n",
    "\n",
    "    assert edge_index.shape == (2, len(ratings_df))\n",
    "    data = HeteroData()\n",
    "    # Add the user nodes:\n",
    "    data['user'].x = user_ids_tensor  # [num_users, num_features_users]\n",
    "    # Add the movie nodes:\n",
    "    data['movie'].x = movie_features  # [num_movies, num_features_movies]\n",
    "    # Add the rating edges:\n",
    "    data['user', 'rates', 'movie'].edge_index = edge_index  # [2, num_ratings]\n",
    "    # Add the rating labels:\n",
    "    rating = torch.from_numpy(ratings_df['rating'].values).to(torch.float)\n",
    "    data['user', 'rates', 'movie'].edge_label = rating  # [num_ratings]\n",
    "\n",
    "    # We also need to make sure to add the reverse edges from movies to users\n",
    "    # in order to let a GNN be able to pass messages in both directions.\n",
    "    # We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    # With the above transformation we also got reversed labels for the edges.\n",
    "    # We are going to remove them:\n",
    "    del data['movie', 'rev_rates', 'user'].edge_label\n",
    "\n",
    "    assert data['user'].num_nodes == len(unique_user_id)\n",
    "    assert data['user', 'rates', 'movie'].num_edges == len(ratings_df)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = create_data(ratings_df_train), create_data(ratings_df_val), create_data(ratings_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (graph_embedding): GraphEmbedding(\n",
      "    (user_embedding): Embedding(610, 150)\n",
      "    (movie_embedding): Embedding(9742, 400)\n",
      "    (movie_embedding_aggregator): Linear(in_features=804, out_features=400, bias=True)\n",
      "  )\n",
      "  (encoder): GraphModule(\n",
      "    (conv1): ModuleDict(\n",
      "      (user__rates__movie): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "      (movie__rev_rates__user): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "    )\n",
      "    (conv2): ModuleDict(\n",
      "      (user__rates__movie): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "      (movie__rev_rates__user): SAGEConv((-1, -1), 32, aggr=mean)\n",
      "    )\n",
      "  )\n",
      "  (decoder): EdgeDecoder(\n",
      "    (lin1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (lin2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GraphEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_users, user_embedding_dim, num_movies, movie_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_embedding = torch.nn.Embedding(num_users, user_embedding_dim)\n",
    "        self.movie_embedding = torch.nn.Embedding(num_movies, movie_embedding_dim)\n",
    "        self.movie_embedding_aggregator = torch.nn.Linear(movie_embedding_dim  + 404, movie_embedding_dim)\n",
    "\n",
    "    def forward(self, x_dict):\n",
    "        x_dict['user'] = self.user_embedding(x_dict['user'])\n",
    "        movie_embedding = self.movie_embedding(torch.LongTensor([int(t.item()) for t in x_dict['movie'][:,0]]).to(device))\n",
    "        movie_other_features = x_dict['movie'][:,1:]\n",
    "        x_dict['movie'] = self.movie_embedding_aggregator(torch.cat((movie_embedding, movie_other_features), dim=1))\n",
    "        # x_dict['movie'] = movie_embedding + movie_other_features\n",
    "        return x_dict\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, user_dim, movie_dim):\n",
    "        super().__init__()\n",
    "        self.graph_embedding = GraphEmbedding(len(user_ids_tensor), user_dim, len(movie_features), movie_dim)\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, train_data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        x_dict = self.graph_embedding(x_dict)\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(hidden_channels=32, user_dim=150, movie_dim=400).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with val_rmse: 3.2960 at epoch 1\n",
      "Epoch: 001, Loss: 13.5477, Train: 3.3206, Val: 3.2960\n",
      "Saving model with val_rmse: 2.5552 at epoch 2\n",
      "Epoch: 002, Loss: 11.0266, Train: 2.5765, Val: 2.5552\n",
      "Saving model with val_rmse: 1.2310 at epoch 3\n",
      "Epoch: 003, Loss: 6.6383, Train: 1.1905, Val: 1.2310\n",
      "Epoch: 004, Loss: 1.4180, Train: 1.8217, Val: 1.8389\n",
      "Epoch: 005, Loss: 16.4489, Train: 1.4136, Val: 1.4348\n",
      "Epoch: 006, Loss: 2.3845, Train: 1.3038, Val: 1.3203\n",
      "Epoch: 007, Loss: 1.7000, Train: 1.9073, Val: 1.8946\n",
      "Epoch: 008, Loss: 3.6378, Train: 2.2102, Val: 2.1912\n",
      "Epoch: 009, Loss: 4.8849, Train: 2.3102, Val: 2.2903\n",
      "Epoch: 010, Loss: 5.3372, Train: 2.2825, Val: 2.2637\n",
      "Epoch: 011, Loss: 5.2098, Train: 2.1554, Val: 2.1391\n",
      "Epoch: 012, Loss: 4.6457, Train: 1.9336, Val: 1.9218\n",
      "Epoch: 013, Loss: 3.7388, Train: 1.6161, Val: 1.6137\n",
      "Epoch: 014, Loss: 2.6118, Train: 1.2421, Val: 1.2636\n",
      "Saving model with val_rmse: 1.1092 at epoch 15\n",
      "Epoch: 015, Loss: 1.5428, Train: 1.0545, Val: 1.1092\n",
      "Epoch: 016, Loss: 1.1290, Train: 1.2826, Val: 1.3209\n",
      "Epoch: 017, Loss: 1.9079, Train: 1.4335, Val: 1.4535\n",
      "Epoch: 018, Loss: 2.5684, Train: 1.3597, Val: 1.3917\n",
      "Epoch: 019, Loss: 2.1116, Train: 1.1475, Val: 1.2077\n",
      "Saving model with val_rmse: 1.0707 at epoch 20\n",
      "Epoch: 020, Loss: 1.3524, Train: 1.0046, Val: 1.0707\n",
      "Epoch: 021, Loss: 1.0099, Train: 1.0405, Val: 1.0880\n",
      "Epoch: 022, Loss: 1.0826, Train: 1.1432, Val: 1.1759\n",
      "Epoch: 023, Loss: 1.3070, Train: 1.2153, Val: 1.2414\n",
      "Epoch: 024, Loss: 1.4769, Train: 1.2264, Val: 1.2525\n",
      "Epoch: 025, Loss: 1.5041, Train: 1.1798, Val: 1.2113\n",
      "Epoch: 026, Loss: 1.3920, Train: 1.0977, Val: 1.1396\n",
      "Epoch: 027, Loss: 1.2049, Train: 1.0195, Val: 1.0759\n",
      "Saving model with val_rmse: 1.0622 at epoch 28\n",
      "Epoch: 028, Loss: 1.0394, Train: 0.9917, Val: 1.0622\n",
      "Epoch: 029, Loss: 0.9836, Train: 1.0277, Val: 1.1051\n",
      "Epoch: 030, Loss: 1.0571, Train: 1.0813, Val: 1.1585\n",
      "Epoch: 031, Loss: 1.1733, Train: 1.0950, Val: 1.1724\n",
      "Epoch: 032, Loss: 1.2047, Train: 1.0552, Val: 1.1359\n",
      "Epoch: 033, Loss: 1.1159, Train: 0.9937, Val: 1.0762\n",
      "Saving model with val_rmse: 1.0348 at epoch 34\n",
      "Epoch: 034, Loss: 0.9879, Train: 0.9564, Val: 1.0348\n",
      "Saving model with val_rmse: 1.0304 at epoch 35\n",
      "Epoch: 035, Loss: 0.9146, Train: 0.9598, Val: 1.0304\n",
      "Epoch: 036, Loss: 0.9213, Train: 0.9847, Val: 1.0479\n",
      "Epoch: 037, Loss: 0.9696, Train: 1.0024, Val: 1.0619\n",
      "Epoch: 038, Loss: 1.0049, Train: 0.9981, Val: 1.0584\n",
      "Epoch: 039, Loss: 0.9963, Train: 0.9741, Val: 1.0393\n",
      "Saving model with val_rmse: 1.0174 at epoch 40\n",
      "Epoch: 040, Loss: 0.9489, Train: 0.9444, Val: 1.0174\n",
      "Saving model with val_rmse: 1.0077 at epoch 41\n",
      "Epoch: 041, Loss: 0.8919, Train: 0.9263, Val: 1.0077\n",
      "Epoch: 042, Loss: 0.8580, Train: 0.9281, Val: 1.0160\n",
      "Epoch: 043, Loss: 0.8614, Train: 0.9419, Val: 1.0333\n",
      "Epoch: 044, Loss: 0.8872, Train: 0.9505, Val: 1.0436\n",
      "Epoch: 045, Loss: 0.9037, Train: 0.9441, Val: 1.0379\n",
      "Epoch: 046, Loss: 0.8915, Train: 0.9271, Val: 1.0206\n",
      "Saving model with val_rmse: 1.0035 at epoch 47\n",
      "Epoch: 047, Loss: 0.8595, Train: 0.9123, Val: 1.0035\n",
      "Saving model with val_rmse: 0.9954 at epoch 48\n",
      "Epoch: 048, Loss: 0.8323, Train: 0.9081, Val: 0.9954\n",
      "Epoch: 049, Loss: 0.8247, Train: 0.9127, Val: 0.9959\n",
      "Epoch: 050, Loss: 0.8330, Train: 0.9180, Val: 0.9988\n",
      "Epoch: 051, Loss: 0.8428, Train: 0.9176, Val: 0.9986\n",
      "Saving model with val_rmse: 0.9941 at epoch 52\n",
      "Epoch: 052, Loss: 0.8420, Train: 0.9104, Val: 0.9941\n",
      "Saving model with val_rmse: 0.9889 at epoch 53\n",
      "Epoch: 053, Loss: 0.8288, Train: 0.9003, Val: 0.9889\n",
      "Saving model with val_rmse: 0.9872 at epoch 54\n",
      "Epoch: 054, Loss: 0.8106, Train: 0.8933, Val: 0.9872\n",
      "Epoch: 055, Loss: 0.7979, Train: 0.8920, Val: 0.9908\n",
      "Epoch: 056, Loss: 0.7957, Train: 0.8945, Val: 0.9967\n",
      "Epoch: 057, Loss: 0.8001, Train: 0.8954, Val: 0.9996\n",
      "Epoch: 058, Loss: 0.8019, Train: 0.8920, Val: 0.9969\n",
      "Epoch: 059, Loss: 0.7956, Train: 0.8855, Val: 0.9900\n",
      "Saving model with val_rmse: 0.9830 at epoch 60\n",
      "Epoch: 060, Loss: 0.7842, Train: 0.8800, Val: 0.9830\n",
      "Saving model with val_rmse: 0.9788 at epoch 61\n",
      "Epoch: 061, Loss: 0.7745, Train: 0.8779, Val: 0.9788\n",
      "Saving model with val_rmse: 0.9774 at epoch 62\n",
      "Epoch: 062, Loss: 0.7707, Train: 0.8782, Val: 0.9774\n",
      "Saving model with val_rmse: 0.9770 at epoch 63\n",
      "Epoch: 063, Loss: 0.7713, Train: 0.8783, Val: 0.9770\n",
      "Saving model with val_rmse: 0.9760 at epoch 64\n",
      "Epoch: 064, Loss: 0.7714, Train: 0.8762, Val: 0.9760\n",
      "Saving model with val_rmse: 0.9746 at epoch 65\n",
      "Epoch: 065, Loss: 0.7678, Train: 0.8723, Val: 0.9746\n",
      "Saving model with val_rmse: 0.9741 at epoch 66\n",
      "Epoch: 066, Loss: 0.7609, Train: 0.8683, Val: 0.9741\n",
      "Epoch: 067, Loss: 0.7539, Train: 0.8659, Val: 0.9753\n",
      "Epoch: 068, Loss: 0.7499, Train: 0.8653, Val: 0.9776\n",
      "Epoch: 069, Loss: 0.7487, Train: 0.8648, Val: 0.9793\n",
      "Epoch: 070, Loss: 0.7479, Train: 0.8631, Val: 0.9787\n",
      "Epoch: 071, Loss: 0.7450, Train: 0.8602, Val: 0.9760\n",
      "Saving model with val_rmse: 0.9725 at epoch 72\n",
      "Epoch: 072, Loss: 0.7399, Train: 0.8572, Val: 0.9725\n",
      "Saving model with val_rmse: 0.9696 at epoch 73\n",
      "Epoch: 073, Loss: 0.7348, Train: 0.8552, Val: 0.9696\n",
      "Saving model with val_rmse: 0.9679 at epoch 74\n",
      "Epoch: 074, Loss: 0.7314, Train: 0.8542, Val: 0.9679\n",
      "Saving model with val_rmse: 0.9668 at epoch 75\n",
      "Epoch: 075, Loss: 0.7296, Train: 0.8532, Val: 0.9668\n",
      "Saving model with val_rmse: 0.9661 at epoch 76\n",
      "Epoch: 076, Loss: 0.7279, Train: 0.8515, Val: 0.9661\n",
      "Saving model with val_rmse: 0.9657 at epoch 77\n",
      "Epoch: 077, Loss: 0.7251, Train: 0.8492, Val: 0.9657\n",
      "Epoch: 078, Loss: 0.7212, Train: 0.8469, Val: 0.9658\n",
      "Epoch: 079, Loss: 0.7173, Train: 0.8452, Val: 0.9666\n",
      "Epoch: 080, Loss: 0.7144, Train: 0.8441, Val: 0.9675\n",
      "Epoch: 081, Loss: 0.7124, Train: 0.8429, Val: 0.9679\n",
      "Epoch: 082, Loss: 0.7105, Train: 0.8413, Val: 0.9672\n",
      "Saving model with val_rmse: 0.9655 at epoch 83\n",
      "Epoch: 083, Loss: 0.7077, Train: 0.8393, Val: 0.9655\n",
      "Saving model with val_rmse: 0.9636 at epoch 84\n",
      "Epoch: 084, Loss: 0.7045, Train: 0.8375, Val: 0.9636\n",
      "Saving model with val_rmse: 0.9619 at epoch 85\n",
      "Epoch: 085, Loss: 0.7014, Train: 0.8361, Val: 0.9619\n",
      "Saving model with val_rmse: 0.9608 at epoch 86\n",
      "Epoch: 086, Loss: 0.6990, Train: 0.8348, Val: 0.9608\n",
      "Saving model with val_rmse: 0.9601 at epoch 87\n",
      "Epoch: 087, Loss: 0.6970, Train: 0.8336, Val: 0.9601\n",
      "Saving model with val_rmse: 0.9597 at epoch 88\n",
      "Epoch: 088, Loss: 0.6948, Train: 0.8320, Val: 0.9597\n",
      "Saving model with val_rmse: 0.9597 at epoch 89\n",
      "Epoch: 089, Loss: 0.6923, Train: 0.8304, Val: 0.9597\n",
      "Epoch: 090, Loss: 0.6896, Train: 0.8289, Val: 0.9600\n",
      "Epoch: 091, Loss: 0.6871, Train: 0.8276, Val: 0.9605\n",
      "Epoch: 092, Loss: 0.6850, Train: 0.8264, Val: 0.9607\n",
      "Epoch: 093, Loss: 0.6830, Train: 0.8251, Val: 0.9604\n",
      "Saving model with val_rmse: 0.9596 at epoch 94\n",
      "Epoch: 094, Loss: 0.6809, Train: 0.8237, Val: 0.9596\n",
      "Saving model with val_rmse: 0.9586 at epoch 95\n",
      "Epoch: 095, Loss: 0.6785, Train: 0.8223, Val: 0.9586\n",
      "Saving model with val_rmse: 0.9576 at epoch 96\n",
      "Epoch: 096, Loss: 0.6762, Train: 0.8211, Val: 0.9576\n",
      "Saving model with val_rmse: 0.9569 at epoch 97\n",
      "Epoch: 097, Loss: 0.6742, Train: 0.8199, Val: 0.9569\n",
      "Saving model with val_rmse: 0.9564 at epoch 98\n",
      "Epoch: 098, Loss: 0.6723, Train: 0.8188, Val: 0.9564\n",
      "Saving model with val_rmse: 0.9563 at epoch 99\n",
      "Epoch: 099, Loss: 0.6704, Train: 0.8175, Val: 0.9563\n",
      "Epoch: 100, Loss: 0.6684, Train: 0.8163, Val: 0.9563\n",
      "Epoch: 101, Loss: 0.6664, Train: 0.8151, Val: 0.9566\n",
      "Epoch: 102, Loss: 0.6644, Train: 0.8140, Val: 0.9569\n",
      "Epoch: 103, Loss: 0.6627, Train: 0.8130, Val: 0.9570\n",
      "Epoch: 104, Loss: 0.6610, Train: 0.8119, Val: 0.9569\n",
      "Epoch: 105, Loss: 0.6593, Train: 0.8108, Val: 0.9565\n",
      "Saving model with val_rmse: 0.9560 at epoch 106\n",
      "Epoch: 106, Loss: 0.6575, Train: 0.8098, Val: 0.9560\n",
      "Saving model with val_rmse: 0.9556 at epoch 107\n",
      "Epoch: 107, Loss: 0.6558, Train: 0.8088, Val: 0.9556\n",
      "Saving model with val_rmse: 0.9552 at epoch 108\n",
      "Epoch: 108, Loss: 0.6542, Train: 0.8079, Val: 0.9552\n",
      "Saving model with val_rmse: 0.9551 at epoch 109\n",
      "Epoch: 109, Loss: 0.6527, Train: 0.8069, Val: 0.9551\n",
      "Epoch: 110, Loss: 0.6512, Train: 0.8060, Val: 0.9552\n",
      "Epoch: 111, Loss: 0.6497, Train: 0.8050, Val: 0.9554\n",
      "Epoch: 112, Loss: 0.6482, Train: 0.8042, Val: 0.9556\n",
      "Epoch: 113, Loss: 0.6467, Train: 0.8033, Val: 0.9558\n",
      "Epoch: 114, Loss: 0.6454, Train: 0.8025, Val: 0.9559\n",
      "Epoch: 115, Loss: 0.6441, Train: 0.8017, Val: 0.9558\n",
      "Epoch: 116, Loss: 0.6428, Train: 0.8009, Val: 0.9557\n",
      "Epoch: 117, Loss: 0.6415, Train: 0.8001, Val: 0.9554\n",
      "Epoch: 118, Loss: 0.6402, Train: 0.7993, Val: 0.9553\n",
      "Epoch: 119, Loss: 0.6390, Train: 0.7986, Val: 0.9553\n",
      "Epoch: 120, Loss: 0.6379, Train: 0.7979, Val: 0.9553\n",
      "Epoch: 121, Loss: 0.6368, Train: 0.7972, Val: 0.9555\n",
      "Epoch: 122, Loss: 0.6357, Train: 0.7965, Val: 0.9557\n",
      "Epoch: 123, Loss: 0.6346, Train: 0.7959, Val: 0.9559\n",
      "Epoch: 124, Loss: 0.6336, Train: 0.7953, Val: 0.9560\n",
      "Epoch: 125, Loss: 0.6326, Train: 0.7947, Val: 0.9560\n",
      "Epoch: 126, Loss: 0.6316, Train: 0.7941, Val: 0.9560\n",
      "Epoch: 127, Loss: 0.6307, Train: 0.7935, Val: 0.9559\n",
      "Epoch: 128, Loss: 0.6298, Train: 0.7930, Val: 0.9559\n",
      "Epoch: 129, Loss: 0.6289, Train: 0.7924, Val: 0.9559\n",
      "Epoch: 130, Loss: 0.6281, Train: 0.7919, Val: 0.9560\n",
      "Epoch: 131, Loss: 0.6273, Train: 0.7914, Val: 0.9562\n",
      "Epoch: 132, Loss: 0.6265, Train: 0.7909, Val: 0.9564\n",
      "Epoch: 133, Loss: 0.6257, Train: 0.7905, Val: 0.9565\n",
      "Epoch: 134, Loss: 0.6250, Train: 0.7900, Val: 0.9566\n",
      "Epoch: 135, Loss: 0.6243, Train: 0.7896, Val: 0.9567\n",
      "Epoch: 136, Loss: 0.6236, Train: 0.7891, Val: 0.9567\n",
      "Epoch: 137, Loss: 0.6229, Train: 0.7887, Val: 0.9567\n",
      "Epoch: 138, Loss: 0.6222, Train: 0.7883, Val: 0.9567\n",
      "Epoch: 139, Loss: 0.6216, Train: 0.7879, Val: 0.9568\n",
      "Epoch: 140, Loss: 0.6210, Train: 0.7876, Val: 0.9570\n",
      "Epoch: 141, Loss: 0.6204, Train: 0.7872, Val: 0.9571\n",
      "Epoch: 142, Loss: 0.6199, Train: 0.7869, Val: 0.9573\n",
      "Epoch: 143, Loss: 0.6193, Train: 0.7865, Val: 0.9574\n",
      "Epoch: 144, Loss: 0.6188, Train: 0.7862, Val: 0.9575\n",
      "Epoch: 145, Loss: 0.6183, Train: 0.7859, Val: 0.9576\n",
      "Epoch: 146, Loss: 0.6178, Train: 0.7856, Val: 0.9577\n",
      "Epoch: 147, Loss: 0.6173, Train: 0.7853, Val: 0.9578\n",
      "Epoch: 148, Loss: 0.6169, Train: 0.7850, Val: 0.9579\n",
      "Epoch: 149, Loss: 0.6164, Train: 0.7847, Val: 0.9580\n",
      "Epoch: 150, Loss: 0.6160, Train: 0.7845, Val: 0.9582\n",
      "Epoch: 151, Loss: 0.6156, Train: 0.7842, Val: 0.9583\n",
      "Epoch: 152, Loss: 0.6152, Train: 0.7839, Val: 0.9585\n",
      "Epoch: 153, Loss: 0.6148, Train: 0.7837, Val: 0.9586\n",
      "Epoch: 154, Loss: 0.6144, Train: 0.7835, Val: 0.9587\n",
      "Epoch: 155, Loss: 0.6140, Train: 0.7832, Val: 0.9588\n",
      "Epoch: 156, Loss: 0.6137, Train: 0.7830, Val: 0.9589\n",
      "Epoch: 157, Loss: 0.6133, Train: 0.7828, Val: 0.9591\n",
      "Epoch: 158, Loss: 0.6130, Train: 0.7826, Val: 0.9592\n",
      "Epoch: 159, Loss: 0.6127, Train: 0.7824, Val: 0.9594\n",
      "Epoch: 160, Loss: 0.6124, Train: 0.7822, Val: 0.9595\n",
      "Epoch: 161, Loss: 0.6121, Train: 0.7820, Val: 0.9596\n",
      "Epoch: 162, Loss: 0.6118, Train: 0.7818, Val: 0.9598\n",
      "Epoch: 163, Loss: 0.6115, Train: 0.7817, Val: 0.9599\n",
      "Epoch: 164, Loss: 0.6112, Train: 0.7815, Val: 0.9600\n",
      "Epoch: 165, Loss: 0.6110, Train: 0.7813, Val: 0.9602\n",
      "Epoch: 166, Loss: 0.6107, Train: 0.7812, Val: 0.9603\n",
      "Epoch: 167, Loss: 0.6105, Train: 0.7810, Val: 0.9605\n",
      "Epoch: 168, Loss: 0.6102, Train: 0.7809, Val: 0.9607\n",
      "Epoch: 169, Loss: 0.6100, Train: 0.7807, Val: 0.9608\n",
      "Epoch: 170, Loss: 0.6097, Train: 0.7806, Val: 0.9609\n",
      "Epoch: 171, Loss: 0.6095, Train: 0.7804, Val: 0.9611\n",
      "Epoch: 172, Loss: 0.6093, Train: 0.7803, Val: 0.9612\n",
      "Epoch: 173, Loss: 0.6091, Train: 0.7802, Val: 0.9613\n",
      "Epoch: 174, Loss: 0.6089, Train: 0.7800, Val: 0.9615\n",
      "Epoch: 175, Loss: 0.6087, Train: 0.7799, Val: 0.9616\n",
      "Epoch: 176, Loss: 0.6085, Train: 0.7798, Val: 0.9618\n",
      "Epoch: 177, Loss: 0.6083, Train: 0.7797, Val: 0.9619\n",
      "Epoch: 178, Loss: 0.6081, Train: 0.7795, Val: 0.9620\n",
      "Epoch: 179, Loss: 0.6079, Train: 0.7794, Val: 0.9621\n",
      "Epoch: 180, Loss: 0.6077, Train: 0.7793, Val: 0.9623\n",
      "Epoch: 181, Loss: 0.6076, Train: 0.7792, Val: 0.9624\n",
      "Epoch: 182, Loss: 0.6074, Train: 0.7791, Val: 0.9625\n",
      "Epoch: 183, Loss: 0.6072, Train: 0.7790, Val: 0.9627\n",
      "Epoch: 184, Loss: 0.6071, Train: 0.7789, Val: 0.9628\n",
      "Epoch: 185, Loss: 0.6069, Train: 0.7788, Val: 0.9630\n",
      "Epoch: 186, Loss: 0.6068, Train: 0.7787, Val: 0.9631\n",
      "Epoch: 187, Loss: 0.6066, Train: 0.7786, Val: 0.9632\n",
      "Epoch: 188, Loss: 0.6065, Train: 0.7785, Val: 0.9634\n",
      "Epoch: 189, Loss: 0.6063, Train: 0.7784, Val: 0.9635\n",
      "Epoch: 190, Loss: 0.6062, Train: 0.7783, Val: 0.9637\n",
      "Epoch: 191, Loss: 0.6061, Train: 0.7783, Val: 0.9638\n",
      "Epoch: 192, Loss: 0.6059, Train: 0.7782, Val: 0.9639\n",
      "Epoch: 193, Loss: 0.6058, Train: 0.7781, Val: 0.9641\n",
      "Epoch: 194, Loss: 0.6057, Train: 0.7780, Val: 0.9642\n",
      "Epoch: 195, Loss: 0.6056, Train: 0.7779, Val: 0.9643\n",
      "Epoch: 196, Loss: 0.6055, Train: 0.7779, Val: 0.9645\n",
      "Epoch: 197, Loss: 0.6053, Train: 0.7778, Val: 0.9646\n",
      "Epoch: 198, Loss: 0.6052, Train: 0.7777, Val: 0.9647\n",
      "Epoch: 199, Loss: 0.6051, Train: 0.7777, Val: 0.9649\n",
      "Epoch: 200, Loss: 0.6050, Train: 0.7776, Val: 0.9650\n",
      "Epoch: 201, Loss: 0.6049, Train: 0.7775, Val: 0.9652\n",
      "Epoch: 202, Loss: 0.6048, Train: 0.7774, Val: 0.9653\n",
      "Epoch: 203, Loss: 0.6047, Train: 0.7774, Val: 0.9655\n",
      "Epoch: 204, Loss: 0.6046, Train: 0.7773, Val: 0.9656\n",
      "Epoch: 205, Loss: 0.6045, Train: 0.7772, Val: 0.9657\n",
      "Epoch: 206, Loss: 0.6044, Train: 0.7772, Val: 0.9659\n",
      "Epoch: 207, Loss: 0.6043, Train: 0.7771, Val: 0.9660\n",
      "Epoch: 208, Loss: 0.6042, Train: 0.7771, Val: 0.9661\n",
      "Epoch: 209, Loss: 0.6041, Train: 0.7770, Val: 0.9662\n",
      "Epoch: 210, Loss: 0.6040, Train: 0.7769, Val: 0.9663\n",
      "Epoch: 211, Loss: 0.6039, Train: 0.7769, Val: 0.9664\n",
      "Epoch: 212, Loss: 0.6038, Train: 0.7768, Val: 0.9666\n",
      "Epoch: 213, Loss: 0.6037, Train: 0.7768, Val: 0.9667\n",
      "Epoch: 214, Loss: 0.6036, Train: 0.7767, Val: 0.9668\n",
      "Epoch: 215, Loss: 0.6035, Train: 0.7767, Val: 0.9669\n",
      "Epoch: 216, Loss: 0.6034, Train: 0.7766, Val: 0.9670\n",
      "Epoch: 217, Loss: 0.6034, Train: 0.7765, Val: 0.9671\n",
      "Epoch: 218, Loss: 0.6033, Train: 0.7765, Val: 0.9672\n",
      "Epoch: 219, Loss: 0.6032, Train: 0.7764, Val: 0.9674\n",
      "Epoch: 220, Loss: 0.6031, Train: 0.7764, Val: 0.9675\n",
      "Epoch: 221, Loss: 0.6030, Train: 0.7763, Val: 0.9676\n",
      "Epoch: 222, Loss: 0.6029, Train: 0.7763, Val: 0.9677\n",
      "Epoch: 223, Loss: 0.6029, Train: 0.7762, Val: 0.9678\n",
      "Epoch: 224, Loss: 0.6028, Train: 0.7762, Val: 0.9679\n",
      "Epoch: 225, Loss: 0.6027, Train: 0.7761, Val: 0.9681\n",
      "Epoch: 226, Loss: 0.6026, Train: 0.7761, Val: 0.9682\n",
      "Epoch: 227, Loss: 0.6026, Train: 0.7760, Val: 0.9683\n",
      "Epoch: 228, Loss: 0.6025, Train: 0.7760, Val: 0.9685\n",
      "Epoch: 229, Loss: 0.6024, Train: 0.7760, Val: 0.9686\n",
      "Epoch: 230, Loss: 0.6024, Train: 0.7759, Val: 0.9687\n",
      "Epoch: 231, Loss: 0.6023, Train: 0.7759, Val: 0.9688\n",
      "Epoch: 232, Loss: 0.6022, Train: 0.7758, Val: 0.9689\n",
      "Epoch: 233, Loss: 0.6022, Train: 0.7758, Val: 0.9690\n",
      "Epoch: 234, Loss: 0.6021, Train: 0.7757, Val: 0.9690\n",
      "Epoch: 235, Loss: 0.6020, Train: 0.7757, Val: 0.9691\n",
      "Epoch: 236, Loss: 0.6020, Train: 0.7757, Val: 0.9692\n",
      "Epoch: 237, Loss: 0.6019, Train: 0.7756, Val: 0.9693\n",
      "Epoch: 238, Loss: 0.6019, Train: 0.7756, Val: 0.9694\n",
      "Epoch: 239, Loss: 0.6018, Train: 0.7756, Val: 0.9695\n",
      "Epoch: 240, Loss: 0.6018, Train: 0.7755, Val: 0.9696\n",
      "Epoch: 241, Loss: 0.6017, Train: 0.7755, Val: 0.9697\n",
      "Epoch: 242, Loss: 0.6017, Train: 0.7755, Val: 0.9698\n",
      "Epoch: 243, Loss: 0.6016, Train: 0.7754, Val: 0.9699\n",
      "Epoch: 244, Loss: 0.6016, Train: 0.7754, Val: 0.9699\n",
      "Epoch: 245, Loss: 0.6015, Train: 0.7754, Val: 0.9700\n",
      "Epoch: 246, Loss: 0.6015, Train: 0.7753, Val: 0.9701\n",
      "Epoch: 247, Loss: 0.6014, Train: 0.7753, Val: 0.9701\n",
      "Epoch: 248, Loss: 0.6014, Train: 0.7753, Val: 0.9702\n",
      "Epoch: 249, Loss: 0.6013, Train: 0.7752, Val: 0.9703\n",
      "Epoch: 250, Loss: 0.6013, Train: 0.7752, Val: 0.9704\n",
      "Epoch: 251, Loss: 0.6012, Train: 0.7752, Val: 0.9705\n",
      "Epoch: 252, Loss: 0.6012, Train: 0.7752, Val: 0.9705\n",
      "Epoch: 253, Loss: 0.6011, Train: 0.7751, Val: 0.9706\n",
      "Epoch: 254, Loss: 0.6011, Train: 0.7751, Val: 0.9707\n",
      "Epoch: 255, Loss: 0.6011, Train: 0.7751, Val: 0.9708\n",
      "Epoch: 256, Loss: 0.6010, Train: 0.7751, Val: 0.9708\n",
      "Epoch: 257, Loss: 0.6010, Train: 0.7750, Val: 0.9709\n",
      "Epoch: 258, Loss: 0.6009, Train: 0.7750, Val: 0.9710\n",
      "Epoch: 259, Loss: 0.6009, Train: 0.7750, Val: 0.9711\n",
      "Epoch: 260, Loss: 0.6009, Train: 0.7750, Val: 0.9711\n",
      "Epoch: 261, Loss: 0.6008, Train: 0.7749, Val: 0.9712\n",
      "Epoch: 262, Loss: 0.6008, Train: 0.7749, Val: 0.9713\n",
      "Epoch: 263, Loss: 0.6008, Train: 0.7749, Val: 0.9714\n",
      "Epoch: 264, Loss: 0.6007, Train: 0.7749, Val: 0.9715\n",
      "Epoch: 265, Loss: 0.6007, Train: 0.7748, Val: 0.9715\n",
      "Epoch: 266, Loss: 0.6006, Train: 0.7748, Val: 0.9716\n",
      "Epoch: 267, Loss: 0.6006, Train: 0.7748, Val: 0.9717\n",
      "Epoch: 268, Loss: 0.6006, Train: 0.7748, Val: 0.9718\n",
      "Epoch: 269, Loss: 0.6005, Train: 0.7748, Val: 0.9719\n",
      "Epoch: 270, Loss: 0.6005, Train: 0.7747, Val: 0.9719\n",
      "Epoch: 271, Loss: 0.6005, Train: 0.7747, Val: 0.9720\n",
      "Epoch: 272, Loss: 0.6004, Train: 0.7747, Val: 0.9721\n",
      "Epoch: 273, Loss: 0.6004, Train: 0.7747, Val: 0.9721\n",
      "Epoch: 274, Loss: 0.6004, Train: 0.7747, Val: 0.9722\n",
      "Epoch: 275, Loss: 0.6003, Train: 0.7746, Val: 0.9723\n",
      "Epoch: 276, Loss: 0.6003, Train: 0.7746, Val: 0.9723\n",
      "Epoch: 277, Loss: 0.6003, Train: 0.7746, Val: 0.9724\n",
      "Epoch: 278, Loss: 0.6003, Train: 0.7746, Val: 0.9725\n",
      "Epoch: 279, Loss: 0.6002, Train: 0.7745, Val: 0.9725\n",
      "Epoch: 280, Loss: 0.6002, Train: 0.7745, Val: 0.9726\n",
      "Epoch: 281, Loss: 0.6002, Train: 0.7745, Val: 0.9726\n",
      "Epoch: 282, Loss: 0.6001, Train: 0.7745, Val: 0.9727\n",
      "Epoch: 283, Loss: 0.6001, Train: 0.7745, Val: 0.9728\n",
      "Epoch: 284, Loss: 0.6001, Train: 0.7745, Val: 0.9728\n",
      "Epoch: 285, Loss: 0.6000, Train: 0.7744, Val: 0.9729\n",
      "Epoch: 286, Loss: 0.6000, Train: 0.7744, Val: 0.9729\n",
      "Epoch: 287, Loss: 0.6000, Train: 0.7744, Val: 0.9730\n",
      "Epoch: 288, Loss: 0.5999, Train: 0.7744, Val: 0.9730\n",
      "Epoch: 289, Loss: 0.5999, Train: 0.7744, Val: 0.9731\n",
      "Epoch: 290, Loss: 0.5999, Train: 0.7743, Val: 0.9731\n",
      "Epoch: 291, Loss: 0.5999, Train: 0.7743, Val: 0.9732\n",
      "Epoch: 292, Loss: 0.5998, Train: 0.7743, Val: 0.9732\n",
      "Epoch: 293, Loss: 0.5998, Train: 0.7743, Val: 0.9733\n",
      "Epoch: 294, Loss: 0.5998, Train: 0.7743, Val: 0.9734\n",
      "Epoch: 295, Loss: 0.5997, Train: 0.7742, Val: 0.9734\n",
      "Epoch: 296, Loss: 0.5997, Train: 0.7742, Val: 0.9734\n",
      "Epoch: 297, Loss: 0.5997, Train: 0.7742, Val: 0.9735\n",
      "Epoch: 298, Loss: 0.5996, Train: 0.7742, Val: 0.9735\n",
      "Epoch: 299, Loss: 0.5996, Train: 0.7742, Val: 0.9736\n",
      "Epoch: 300, Loss: 0.5996, Train: 0.7741, Val: 0.9737\n",
      "Epoch: 301, Loss: 0.5996, Train: 0.7741, Val: 0.9737\n",
      "Epoch: 302, Loss: 0.5995, Train: 0.7741, Val: 0.9737\n",
      "Epoch: 303, Loss: 0.5995, Train: 0.7741, Val: 0.9738\n",
      "Epoch: 304, Loss: 0.5995, Train: 0.7741, Val: 0.9739\n",
      "Epoch: 305, Loss: 0.5994, Train: 0.7740, Val: 0.9739\n",
      "Epoch: 306, Loss: 0.5994, Train: 0.7740, Val: 0.9739\n",
      "Epoch: 307, Loss: 0.5994, Train: 0.7740, Val: 0.9740\n",
      "Epoch: 308, Loss: 0.5994, Train: 0.7740, Val: 0.9741\n",
      "Epoch: 309, Loss: 0.5993, Train: 0.7740, Val: 0.9741\n",
      "Epoch: 310, Loss: 0.5993, Train: 0.7739, Val: 0.9741\n",
      "Epoch: 311, Loss: 0.5993, Train: 0.7739, Val: 0.9742\n",
      "Epoch: 312, Loss: 0.5992, Train: 0.7739, Val: 0.9742\n",
      "Epoch: 313, Loss: 0.5992, Train: 0.7739, Val: 0.9743\n",
      "Epoch: 314, Loss: 0.5992, Train: 0.7739, Val: 0.9743\n",
      "Epoch: 315, Loss: 0.5991, Train: 0.7739, Val: 0.9743\n",
      "Epoch: 316, Loss: 0.5991, Train: 0.7738, Val: 0.9744\n",
      "Epoch: 317, Loss: 0.5991, Train: 0.7738, Val: 0.9745\n",
      "Epoch: 318, Loss: 0.5990, Train: 0.7738, Val: 0.9745\n",
      "Epoch: 319, Loss: 0.5990, Train: 0.7738, Val: 0.9745\n",
      "Epoch: 320, Loss: 0.5990, Train: 0.7738, Val: 0.9746\n",
      "Epoch: 321, Loss: 0.5990, Train: 0.7737, Val: 0.9746\n",
      "Epoch: 322, Loss: 0.5989, Train: 0.7737, Val: 0.9746\n",
      "Epoch: 323, Loss: 0.5989, Train: 0.7737, Val: 0.9747\n",
      "Epoch: 324, Loss: 0.5989, Train: 0.7737, Val: 0.9747\n",
      "Epoch: 325, Loss: 0.5988, Train: 0.7737, Val: 0.9747\n",
      "Epoch: 326, Loss: 0.5988, Train: 0.7736, Val: 0.9748\n",
      "Epoch: 327, Loss: 0.5988, Train: 0.7736, Val: 0.9748\n",
      "Epoch: 328, Loss: 0.5987, Train: 0.7736, Val: 0.9749\n",
      "Epoch: 329, Loss: 0.5987, Train: 0.7736, Val: 0.9749\n",
      "Epoch: 330, Loss: 0.5987, Train: 0.7736, Val: 0.9749\n",
      "Epoch: 331, Loss: 0.5986, Train: 0.7735, Val: 0.9749\n",
      "Epoch: 332, Loss: 0.5986, Train: 0.7735, Val: 0.9750\n",
      "Epoch: 333, Loss: 0.5986, Train: 0.7735, Val: 0.9750\n",
      "Epoch: 334, Loss: 0.5985, Train: 0.7735, Val: 0.9750\n",
      "Epoch: 335, Loss: 0.5985, Train: 0.7734, Val: 0.9751\n",
      "Epoch: 336, Loss: 0.5985, Train: 0.7734, Val: 0.9751\n",
      "Epoch: 337, Loss: 0.5984, Train: 0.7734, Val: 0.9752\n",
      "Epoch: 338, Loss: 0.5984, Train: 0.7734, Val: 0.9752\n",
      "Epoch: 339, Loss: 0.5984, Train: 0.7734, Val: 0.9753\n",
      "Epoch: 340, Loss: 0.5983, Train: 0.7733, Val: 0.9753\n",
      "Epoch: 341, Loss: 0.5983, Train: 0.7733, Val: 0.9754\n",
      "Epoch: 342, Loss: 0.5983, Train: 0.7733, Val: 0.9753\n",
      "Epoch: 343, Loss: 0.5982, Train: 0.7733, Val: 0.9754\n",
      "Epoch: 344, Loss: 0.5982, Train: 0.7733, Val: 0.9755\n",
      "Epoch: 345, Loss: 0.5982, Train: 0.7732, Val: 0.9754\n",
      "Epoch: 346, Loss: 0.5981, Train: 0.7732, Val: 0.9756\n",
      "Epoch: 347, Loss: 0.5981, Train: 0.7732, Val: 0.9755\n",
      "Epoch: 348, Loss: 0.5981, Train: 0.7732, Val: 0.9755\n",
      "Epoch: 349, Loss: 0.5980, Train: 0.7731, Val: 0.9757\n",
      "Epoch: 350, Loss: 0.5980, Train: 0.7731, Val: 0.9756\n",
      "Epoch: 351, Loss: 0.5980, Train: 0.7731, Val: 0.9757\n",
      "Epoch: 352, Loss: 0.5979, Train: 0.7731, Val: 0.9757\n",
      "Epoch: 353, Loss: 0.5979, Train: 0.7730, Val: 0.9757\n",
      "Epoch: 354, Loss: 0.5978, Train: 0.7730, Val: 0.9758\n",
      "Epoch: 355, Loss: 0.5978, Train: 0.7730, Val: 0.9759\n",
      "Epoch: 356, Loss: 0.5978, Train: 0.7730, Val: 0.9758\n",
      "Epoch: 357, Loss: 0.5977, Train: 0.7729, Val: 0.9759\n",
      "Epoch: 358, Loss: 0.5977, Train: 0.7729, Val: 0.9759\n",
      "Epoch: 359, Loss: 0.5977, Train: 0.7729, Val: 0.9759\n",
      "Epoch: 360, Loss: 0.5976, Train: 0.7729, Val: 0.9760\n",
      "Epoch: 361, Loss: 0.5976, Train: 0.7728, Val: 0.9760\n",
      "Epoch: 362, Loss: 0.5975, Train: 0.7728, Val: 0.9761\n",
      "Epoch: 363, Loss: 0.5975, Train: 0.7728, Val: 0.9760\n",
      "Epoch: 364, Loss: 0.5975, Train: 0.7728, Val: 0.9761\n",
      "Epoch: 365, Loss: 0.5974, Train: 0.7727, Val: 0.9762\n",
      "Epoch: 366, Loss: 0.5974, Train: 0.7727, Val: 0.9761\n",
      "Epoch: 367, Loss: 0.5973, Train: 0.7727, Val: 0.9762\n",
      "Epoch: 368, Loss: 0.5973, Train: 0.7727, Val: 0.9763\n",
      "Epoch: 369, Loss: 0.5973, Train: 0.7726, Val: 0.9762\n",
      "Epoch: 370, Loss: 0.5972, Train: 0.7726, Val: 0.9762\n",
      "Epoch: 371, Loss: 0.5972, Train: 0.7726, Val: 0.9764\n",
      "Epoch: 372, Loss: 0.5971, Train: 0.7726, Val: 0.9764\n",
      "Epoch: 373, Loss: 0.5971, Train: 0.7725, Val: 0.9763\n",
      "Epoch: 374, Loss: 0.5970, Train: 0.7725, Val: 0.9765\n",
      "Epoch: 375, Loss: 0.5970, Train: 0.7725, Val: 0.9764\n",
      "Epoch: 376, Loss: 0.5970, Train: 0.7724, Val: 0.9764\n",
      "Epoch: 377, Loss: 0.5969, Train: 0.7724, Val: 0.9766\n",
      "Epoch: 378, Loss: 0.5969, Train: 0.7724, Val: 0.9765\n",
      "Epoch: 379, Loss: 0.5968, Train: 0.7724, Val: 0.9765\n",
      "Epoch: 380, Loss: 0.5968, Train: 0.7723, Val: 0.9767\n",
      "Epoch: 381, Loss: 0.5967, Train: 0.7723, Val: 0.9766\n",
      "Epoch: 382, Loss: 0.5967, Train: 0.7723, Val: 0.9767\n",
      "Epoch: 383, Loss: 0.5966, Train: 0.7722, Val: 0.9768\n",
      "Epoch: 384, Loss: 0.5966, Train: 0.7722, Val: 0.9767\n",
      "Epoch: 385, Loss: 0.5965, Train: 0.7722, Val: 0.9769\n",
      "Epoch: 386, Loss: 0.5965, Train: 0.7721, Val: 0.9767\n",
      "Epoch: 387, Loss: 0.5964, Train: 0.7721, Val: 0.9769\n",
      "Epoch: 388, Loss: 0.5964, Train: 0.7721, Val: 0.9770\n",
      "Epoch: 389, Loss: 0.5963, Train: 0.7720, Val: 0.9768\n",
      "Epoch: 390, Loss: 0.5963, Train: 0.7720, Val: 0.9770\n",
      "Epoch: 391, Loss: 0.5962, Train: 0.7720, Val: 0.9771\n",
      "Epoch: 392, Loss: 0.5962, Train: 0.7719, Val: 0.9769\n",
      "Epoch: 393, Loss: 0.5961, Train: 0.7719, Val: 0.9771\n",
      "Epoch: 394, Loss: 0.5961, Train: 0.7719, Val: 0.9773\n",
      "Epoch: 395, Loss: 0.5960, Train: 0.7718, Val: 0.9770\n",
      "Epoch: 396, Loss: 0.5960, Train: 0.7718, Val: 0.9771\n",
      "Epoch: 397, Loss: 0.5959, Train: 0.7717, Val: 0.9774\n",
      "Epoch: 398, Loss: 0.5958, Train: 0.7717, Val: 0.9772\n",
      "Epoch: 399, Loss: 0.5958, Train: 0.7717, Val: 0.9771\n",
      "Epoch: 400, Loss: 0.5957, Train: 0.7716, Val: 0.9773\n",
      "Epoch: 401, Loss: 0.5956, Train: 0.7716, Val: 0.9774\n",
      "Epoch: 402, Loss: 0.5956, Train: 0.7715, Val: 0.9772\n",
      "Epoch: 403, Loss: 0.5955, Train: 0.7715, Val: 0.9774\n",
      "Epoch: 404, Loss: 0.5954, Train: 0.7714, Val: 0.9775\n",
      "Epoch: 405, Loss: 0.5954, Train: 0.7714, Val: 0.9772\n",
      "Epoch: 406, Loss: 0.5953, Train: 0.7714, Val: 0.9774\n",
      "Epoch: 407, Loss: 0.5952, Train: 0.7713, Val: 0.9776\n",
      "Epoch: 408, Loss: 0.5952, Train: 0.7713, Val: 0.9772\n",
      "Epoch: 409, Loss: 0.5951, Train: 0.7712, Val: 0.9774\n",
      "Epoch: 410, Loss: 0.5950, Train: 0.7712, Val: 0.9777\n",
      "Epoch: 411, Loss: 0.5950, Train: 0.7711, Val: 0.9773\n",
      "Epoch: 412, Loss: 0.5949, Train: 0.7711, Val: 0.9773\n",
      "Epoch: 413, Loss: 0.5948, Train: 0.7710, Val: 0.9777\n",
      "Epoch: 414, Loss: 0.5947, Train: 0.7710, Val: 0.9775\n",
      "Epoch: 415, Loss: 0.5946, Train: 0.7709, Val: 0.9772\n",
      "Epoch: 416, Loss: 0.5946, Train: 0.7709, Val: 0.9776\n",
      "Epoch: 417, Loss: 0.5945, Train: 0.7708, Val: 0.9776\n",
      "Epoch: 418, Loss: 0.5944, Train: 0.7708, Val: 0.9773\n",
      "Epoch: 419, Loss: 0.5943, Train: 0.7707, Val: 0.9775\n",
      "Epoch: 420, Loss: 0.5942, Train: 0.7707, Val: 0.9778\n",
      "Epoch: 421, Loss: 0.5941, Train: 0.7706, Val: 0.9773\n",
      "Epoch: 422, Loss: 0.5941, Train: 0.7705, Val: 0.9774\n",
      "Epoch: 423, Loss: 0.5940, Train: 0.7705, Val: 0.9779\n",
      "Epoch: 424, Loss: 0.5939, Train: 0.7704, Val: 0.9774\n",
      "Epoch: 425, Loss: 0.5938, Train: 0.7703, Val: 0.9775\n",
      "Epoch: 426, Loss: 0.5937, Train: 0.7703, Val: 0.9779\n",
      "Epoch: 427, Loss: 0.5936, Train: 0.7702, Val: 0.9777\n",
      "Epoch: 428, Loss: 0.5935, Train: 0.7702, Val: 0.9775\n",
      "Epoch: 429, Loss: 0.5934, Train: 0.7701, Val: 0.9778\n",
      "Epoch: 430, Loss: 0.5933, Train: 0.7700, Val: 0.9776\n",
      "Epoch: 431, Loss: 0.5932, Train: 0.7699, Val: 0.9776\n",
      "Epoch: 432, Loss: 0.5930, Train: 0.7699, Val: 0.9777\n",
      "Epoch: 433, Loss: 0.5929, Train: 0.7698, Val: 0.9774\n",
      "Epoch: 434, Loss: 0.5928, Train: 0.7697, Val: 0.9779\n",
      "Epoch: 435, Loss: 0.5927, Train: 0.7697, Val: 0.9774\n",
      "Epoch: 436, Loss: 0.5926, Train: 0.7696, Val: 0.9779\n",
      "Epoch: 437, Loss: 0.5925, Train: 0.7695, Val: 0.9779\n",
      "Epoch: 438, Loss: 0.5924, Train: 0.7694, Val: 0.9776\n",
      "Epoch: 439, Loss: 0.5922, Train: 0.7693, Val: 0.9779\n",
      "Epoch: 440, Loss: 0.5921, Train: 0.7692, Val: 0.9777\n",
      "Epoch: 441, Loss: 0.5920, Train: 0.7692, Val: 0.9779\n",
      "Epoch: 442, Loss: 0.5918, Train: 0.7691, Val: 0.9778\n",
      "Epoch: 443, Loss: 0.5917, Train: 0.7690, Val: 0.9781\n",
      "Epoch: 444, Loss: 0.5916, Train: 0.7689, Val: 0.9776\n",
      "Epoch: 445, Loss: 0.5915, Train: 0.7688, Val: 0.9783\n",
      "Epoch: 446, Loss: 0.5913, Train: 0.7687, Val: 0.9777\n",
      "Epoch: 447, Loss: 0.5912, Train: 0.7686, Val: 0.9782\n",
      "Epoch: 448, Loss: 0.5910, Train: 0.7685, Val: 0.9782\n",
      "Epoch: 449, Loss: 0.5908, Train: 0.7684, Val: 0.9779\n",
      "Epoch: 450, Loss: 0.5907, Train: 0.7683, Val: 0.9784\n",
      "Epoch: 451, Loss: 0.5905, Train: 0.7682, Val: 0.9778\n",
      "Epoch: 452, Loss: 0.5904, Train: 0.7681, Val: 0.9786\n",
      "Epoch: 453, Loss: 0.5902, Train: 0.7680, Val: 0.9780\n",
      "Epoch: 454, Loss: 0.5901, Train: 0.7679, Val: 0.9786\n",
      "Epoch: 455, Loss: 0.5899, Train: 0.7678, Val: 0.9786\n",
      "Epoch: 456, Loss: 0.5897, Train: 0.7677, Val: 0.9782\n",
      "Epoch: 457, Loss: 0.5896, Train: 0.7676, Val: 0.9789\n",
      "Epoch: 458, Loss: 0.5894, Train: 0.7675, Val: 0.9777\n",
      "Epoch: 459, Loss: 0.5893, Train: 0.7674, Val: 0.9790\n",
      "Epoch: 460, Loss: 0.5891, Train: 0.7672, Val: 0.9785\n",
      "Epoch: 461, Loss: 0.5888, Train: 0.7671, Val: 0.9786\n",
      "Epoch: 462, Loss: 0.5887, Train: 0.7670, Val: 0.9791\n",
      "Epoch: 463, Loss: 0.5885, Train: 0.7669, Val: 0.9782\n",
      "Epoch: 464, Loss: 0.5883, Train: 0.7667, Val: 0.9792\n",
      "Epoch: 465, Loss: 0.5881, Train: 0.7666, Val: 0.9785\n",
      "Epoch: 466, Loss: 0.5879, Train: 0.7665, Val: 0.9792\n",
      "Epoch: 467, Loss: 0.5877, Train: 0.7664, Val: 0.9790\n",
      "Epoch: 468, Loss: 0.5875, Train: 0.7662, Val: 0.9790\n",
      "Epoch: 469, Loss: 0.5873, Train: 0.7661, Val: 0.9792\n",
      "Epoch: 470, Loss: 0.5871, Train: 0.7660, Val: 0.9790\n",
      "Epoch: 471, Loss: 0.5869, Train: 0.7659, Val: 0.9798\n",
      "Epoch: 472, Loss: 0.5868, Train: 0.7658, Val: 0.9781\n",
      "Epoch: 473, Loss: 0.5867, Train: 0.7656, Val: 0.9803\n",
      "Epoch: 474, Loss: 0.5864, Train: 0.7654, Val: 0.9792\n",
      "Epoch: 475, Loss: 0.5861, Train: 0.7653, Val: 0.9796\n",
      "Epoch: 476, Loss: 0.5859, Train: 0.7652, Val: 0.9805\n",
      "Epoch: 477, Loss: 0.5857, Train: 0.7651, Val: 0.9789\n",
      "Epoch: 478, Loss: 0.5856, Train: 0.7649, Val: 0.9808\n",
      "Epoch: 479, Loss: 0.5853, Train: 0.7647, Val: 0.9801\n",
      "Epoch: 480, Loss: 0.5850, Train: 0.7646, Val: 0.9802\n",
      "Epoch: 481, Loss: 0.5848, Train: 0.7645, Val: 0.9812\n",
      "Epoch: 482, Loss: 0.5846, Train: 0.7644, Val: 0.9794\n",
      "Epoch: 483, Loss: 0.5845, Train: 0.7642, Val: 0.9816\n",
      "Epoch: 484, Loss: 0.5842, Train: 0.7640, Val: 0.9806\n",
      "Epoch: 485, Loss: 0.5839, Train: 0.7638, Val: 0.9812\n",
      "Epoch: 486, Loss: 0.5836, Train: 0.7637, Val: 0.9814\n",
      "Epoch: 487, Loss: 0.5834, Train: 0.7635, Val: 0.9809\n",
      "Epoch: 488, Loss: 0.5832, Train: 0.7634, Val: 0.9821\n",
      "Epoch: 489, Loss: 0.5830, Train: 0.7633, Val: 0.9803\n",
      "Epoch: 490, Loss: 0.5829, Train: 0.7632, Val: 0.9830\n",
      "Epoch: 491, Loss: 0.5827, Train: 0.7631, Val: 0.9803\n",
      "Epoch: 492, Loss: 0.5825, Train: 0.7628, Val: 0.9826\n",
      "Epoch: 493, Loss: 0.5820, Train: 0.7626, Val: 0.9825\n",
      "Epoch: 494, Loss: 0.5818, Train: 0.7625, Val: 0.9811\n",
      "Epoch: 495, Loss: 0.5816, Train: 0.7623, Val: 0.9831\n",
      "Epoch: 496, Loss: 0.5813, Train: 0.7621, Val: 0.9823\n",
      "Epoch: 497, Loss: 0.5810, Train: 0.7619, Val: 0.9823\n",
      "Epoch: 498, Loss: 0.5807, Train: 0.7618, Val: 0.9832\n",
      "Epoch: 499, Loss: 0.5805, Train: 0.7617, Val: 0.9817\n",
      "Epoch: 500, Loss: 0.5804, Train: 0.7616, Val: 0.9844\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Define training function\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['user', 'movie'].edge_index)\n",
    "    target = train_data['user', 'movie'].edge_label\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "# Define test function\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)\n",
    "\n",
    "best_val_rmse = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 501):\n",
    "    train_data = train_data.to(device) # type: ignore\n",
    "    loss = train(model)\n",
    "    train_rmse = test(model, train_data)\n",
    "    val_rmse = test(model, val_data)\n",
    "\n",
    "    # Check if the current validation RMSE is the best\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        print(f'Saving model with val_rmse: {val_rmse:.4f} at epoch {epoch}')\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, Val: {val_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with val_rmse: 0.9551 at epoch 109\n",
      "Test rmse: 0.9375741481781006\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading model with val_rmse: {best_val_rmse:.4f} at epoch {best_epoch}')\n",
    "model.load_state_dict(torch.load(model_file_name))\n",
    "\n",
    "print(\"Test rmse:\", test(model, test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
